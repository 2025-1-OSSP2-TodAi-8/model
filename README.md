## ğŸ“  í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°ì • ë¶„ì„ ëª¨ë¸

[![ğŸ¤— Hugging Face](https://img.shields.io/badge/HuggingFace-Text%20Emotion%20Model-yellow)](https://huggingface.co/HyukII/text-emotion-model)


### ğŸ“Œ 1. ê°œìš”
- ì‚¬ìš©ìê°€ ì‘ì„±í•˜ê±°ë‚˜ ìŒì„±ì—ì„œ ë³€í™˜ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸.
- í•œêµ­ì–´ì— íŠ¹í™”ëœ klue/roberta-base ì‚¬ì „í•™ìŠµ ì–¸ì–´ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ì—¬ êµ¬í˜„ë˜ì—ˆë‹¤.
- ì•±ì—ì„œ ë…¹ìŒëœ ìŒì„±ì€ STT(Speech-to-Text) ê³¼ì •ì„ ê±°ì³ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜ë˜ë©°, íƒìŠ¤íŠ¸ ëª¨ë¸ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ì‚¬ìš©ëœë‹¤.

### ğŸ” 2. ëª¨ë¸ êµ¬ì¡°
- êµ¬ì¡°: Transformer ê¸°ë°˜ ì‚¬ì „í•™ìŠµ ëª¨ë¸(klue/roberta-base)
- Embedding & Encoder: ì…ë ¥ ë¬¸ì¥ì„ í† í°í™”í•˜ì—¬ Transformer ì¸ì½”ë”ë¡œ íŠ¹ì§• ì¶”ì¶œ
- ì¶œë ¥: 6ê°œ ê°ì • í´ë˜ìŠ¤ í™•ë¥  (ANGRY, SAD, DISGUST, HAPPY, FEAR, SURPRISE)

### âš™ï¸ 3. í•™ìŠµ ë°©ë²•
3.1. ë°ì´í„°ì…‹ êµ¬ì„±
- í›ˆë ¨ ë°ì´í„° : AiHubì˜ 'ê³µê°í˜• ëŒ€í™”' (https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=7130)
- ì „ì²˜ë¦¬: ìµœëŒ€ ê¸¸ì´ 256 í† í°ìœ¼ë¡œ í† í°í™”
- ë¶ˆê· í˜• ë³´ì™„: í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©
  
3.2. í›ˆë ¨ ì „ëµ
- ì…ë ¥: í† í° ID ì‹œí€€ìŠ¤ (max_length=256)
- ì¶œë ¥: ê°ì • í™•ë¥  ë¶„í¬ (Softmax)
- ì†ì‹¤ í•¨ìˆ˜: CrossEntropyLoss (í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì ìš©)
- ìµœì í™” ê¸°ë²•: AdamW (lr=2e-5, weight_decay=0.01)
- í›ˆë ¨ ì„¤ì •: batch size=16, epoch 5~6, Early Stopping(patience=2)
- ì •ê·œí™”: Dropout=0.1, max_grad_norm=1.0


<img width="500" height="400" alt="image" src="https://github.com/user-attachments/assets/aba3f228-801c-45a2-89d0-2c63d745a173" />

### 4. ëª¨ë¸ ì‚¬ìš© 
#### 4.1. ë¬¸ì¥ ë‹¨ìœ„ ì§‘ê³„ ì½”ë“œ(ë¬¸ì¥ ë§ˆë‹¤ ì˜ˆì¸¡ -> ê°œìˆ˜ ë¹„ìœ¨ë¡œ í¼ì„¼íŠ¸ ê³„ì‚°)
```python
def split_sents(text):
    # ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ/ì¤„ë°”ê¿ˆ ê¸°ì¤€
    return [s.strip() for s in re.split(r'[.?!\n]', text) if s.strip()]

def analyze_diary_percent(diary_text, max_len=256, return_details=False):
    sents = split_sents(diary_text)
    if not sents:
        print("ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤."); return {}

    counts = {id2label[i]: 0 for i in range(num_labels)}
    details = []

    with torch.no_grad():
        for s in sents:
            enc = tok(s, truncation=True, padding=True, max_length=max_len, return_tensors="pt").to(device)
            logits = model(**enc).logits
            pred = int(logits.argmax(-1).cpu().numpy()[0])
            lab = id2label[pred]
            counts[lab] += 1
            if return_details: details.append((s, lab))

    total = sum(counts.values())
    perc = {lab: round((counts.get(lab, 0) / total) * 100, 2) if total > 0 else 0.0 for lab in id2label.values()}

    print("=== í…ìŠ¤íŠ¸ ê¸°ë°˜ ê°ì • ë¶„ì„ ===")
    for lab, pct in sorted(perc.items(), key=lambda x: -x[1]):
        print(f"{lab:<5}: {pct:5.2f}% ")
    print("============================")


```

#### 4.3 ëª¨ë¸ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤
- ëª¨ë¸ì€ ì¼ê¸° í…ìŠ¤íŠ¸ì˜ í•œì¤„ í•œì¤„ì„ ë°›ì•„ ê°ì •ì„ ë¶„ì„í•œë‹¤
- ì´ ì¼ê¸° í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ëŠì€ í›„ ë¬¸ì¥ ë§ˆë‹¤ ê°ì •ì„ ë¶„ì„í•œ í›„ ì´ ì¼ê¸° ë‚´ìš©ì˜ ê°ì •ì„ ìˆ˜ì¹˜í™” í•œë‹¤ 
- analyze_diary_percent(diary_text)  (diary_text : ì¼ê¸° ë‚´ìš©)


### ğŸ”¥ Model card: 
**HyukII/text-emotion-model**

### ğŸ”¥ Load in code:

```python
from transformers import AutoTokenizer, AutoModelForSequenceClassification
tok = AutoTokenizer.from_pretrained("HyukII/text-emotion-model")
model = AutoModelForSequenceClassification.from_pretrained("HyukII/text-emotion-model").eval()
```
---
## ğŸ¤ ì˜¤ë””ì˜¤ ê¸°ë°˜ ê°ì • ë¶„ì„ ëª¨ë¸
[![ğŸ¤— Model on HF](https://img.shields.io/badge/HuggingFace-Audio%20Emotion%20Model-yellow)](https://huggingface.co/HyukII/audio-emotion-model)

### ğŸ“Œ 1. ê°œìš”
- ì‚¬ìš©ìì˜ ë…¹ìŒ ìŒì„±ì„ ì…ë ¥ë°›ì•„ ìŒí–¥ì  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³  ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸
- í…ìŠ¤íŠ¸ ë¶„ì„ ëª¨ë¸ê³¼ ë‹¬ë¦¬, ëª©ì†Œë¦¬ì˜ ì–µì–‘Â·ì†ë„Â·ì—ë„ˆì§€Â·ìŠ¤í™íŠ¸ëŸ¼ ë³€í™” ë“±ì„ í™œìš©í•´ ê°ì •ì„ ê°ì§€
- ì¼ê¸° í…ìŠ¤íŠ¸ê°€ ê¸ì •ì ìœ¼ë¡œ ì‘ì„±ë˜ë”ë¼ë„, ëª©ì†Œë¦¬ í†¤ì´ ìš°ìš¸í•˜ë‹¤ë©´ ì‹¤ì œ ê°ì •ì„ ë³´ì™„ì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆìŒ

### ğŸ› ï¸ 2. íŠ¹ì§• ì¶”ì¶œ (Feature Extraction) 
ë³¸ í”„ë¡œì íŠ¸ì—ì„œëŠ” ìŒì„±ì—ì„œ MFCC(Mel-Frequency Cepstral Coefficients)ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê³  ìˆìŒ
- MFCC (Mel-Frequency Cepstral Coefficients): ìŒì„±ì˜ ì£¼íŒŒìˆ˜ ìŠ¤í™íŠ¸ëŸ¼ì„ ìš”ì•½í•œ 13ì°¨ì› ê³„ìˆ˜
- ê³ ì •ëœ ì‹œí€€ìŠ¤ ê¸¸ì´: 100 í”„ë ˆì„ìœ¼ë¡œ ë§ì¶”ì–´ CNN-LSTM ëª¨ë¸ì— ì…ë ¥ ê°€ëŠ¥
- ì‚¬ìš© ë¼ì´ë¸ŒëŸ¬ë¦¬: librosa, numpy

### ğŸ” 3. ëª¨ë¸ êµ¬ì¡°
- CNN + BiLSTM ê¸°ë°˜ ì‹œí€€ìŠ¤ ëª¨ë¸
- Conv1D â†’ ìŒí–¥ ìŠ¤í™íŠ¸ëŸ¼ íŠ¹ì§• ì¶”ì¶œ
- BiLSTM â†’ ì‹œê°„ì  ë³€í™” íŒ¨í„´ í•™ìŠµ
- Dense + Softmax â†’ ê°ì • í´ë˜ìŠ¤ í™•ë¥  ì¶œë ¥


### âš™ï¸ 4.í•™ìŠµ ë°©ë²•
4.1. ë°ì´í„°ì…‹ êµ¬ì„±
- í›ˆë ¨ ë°ì´í„° : AiHubì˜ 'ê°ì • ìŒì„± ë°ì´í„°ì…‹' (https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&dataSetSn=637)
- ì‚¬ìš©ì ìŒì„± â†’ 13ì°¨ì› íŠ¹ì§• ë²¡í„°(MFCC ë“±) ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
- ë ˆì´ë¸”: ê°ì • í´ë˜ìŠ¤ (ì˜ˆ: JOY, SAD, ANGRY ë“±)
- ë¶ˆê· í˜• ë³´ì™„: ë°ì´í„° ì¦ê°•(ì†ë„ ë³€í™˜, í”¼ì¹˜ ì‰¬í”„íŠ¸)

4.2. í›ˆë ¨ ì „ëµ
- ì…ë ¥: (ì‹œí€€ìŠ¤ ê¸¸ì´, feature_dim) í˜•íƒœì˜ ìŒì„± íŠ¹ì§• ì‹œí€€ìŠ¤
- ì¶œë ¥: ê°ì • í™•ë¥  ë¶„í¬ (Softmax)
- ì†ì‹¤ í•¨ìˆ˜: CrossEntropyLoss
- ì˜µí‹°ë§ˆì´ì €: AdamW, learning rate scheduling ì ìš©

4.3. ì¤‘ë¦½ ë²¡í„° ê¸°ë°˜ ë¶„ì„ (Delta Approach)
- ë‚¨ì, ì—¬ììš©  Neutral Baseline Vectorë¥¼ ë¨¼ì € ì €ì¥
- ìƒˆë¡œìš´ ë°œí™” ì…ë ¥ ì‹œ â†’ Î” = (í˜„ì¬ ë²¡í„° â€“ baseline) ê³„ì‚°
- Î” ë²¡í„°ë¥¼ ëª¨ë¸ì— ì…ë ¥í•˜ì—¬ ê°œì¸í™”ëœ ê°ì • ì˜ˆì¸¡ ê°€ëŠ¥


#### ğŸ”Š ìŒì„± íŒŒì¼ ë“£ê¸°
[M0001_114169.wav](M0001_114169.wav)

<img width="400" height="300" alt="image" src="https://github.com/user-attachments/assets/efd6f2a2-1d85-4fba-8519-0ba57760f3b5" />


### 5. ëª¨ë¸ ì‚¬ìš©
#### 5.1 ì‹œí€€ìŠ¤ ìŒì„± íŒŒì¼ ë§Œë“œëŠ” ì½”ë“œ
```python
def extract_sequence_features(wav_path, max_len=100): #wav_path = ìŒì„±íŒŒì¼ëª…
    y, sr = librosa.load(wav_path, sr=None)
    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13).T  # (time, 13)
    if len(mfcc) < max_len:
        pad_width = max_len - len(mfcc)
        mfcc = np.pad(mfcc, ((0, pad_width), (0, 0)), mode='constant')
    else:
        mfcc = mfcc[:max_len]
    return mfcc
```

#### 5.2.ë² ì´ìŠ¤ë¼ì¸ ë²¡í„° ì¶”ì¶œí•˜ëŠ” ì½”ë“œ 

```python
def compute_baseline_vectors(file_paths):
    all_vectors = []
    for path in file_paths:
        seq = extract_sequence_features(path)  # shape (100, 13)
        mean_vec = np.mean(seq, axis=0)        # shape (13,)
        all_vectors.append(mean_vec)
    all_vectors = np.stack(all_vectors)        # shape (15, 13)

    baseline_mean = np.mean(all_vectors, axis=0)
    baseline_std = np.std(all_vectors, axis=0)

    return baseline_mean, baseline_std
```
#### 5.3. ëª¨ë¸ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤
- 5.1ê³¼ 5.2 ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¤‘ë¦½ìŒì„±ìš© ì‹œí€€ìŠ¤ë² ì´ìŠ¤ë²¡í„°ë¥¼ ë§Œë“ ë‹¤ => ë² ì´ìŠ¤ë²¡í„° í‰ê· , ë² ì´ìŠ¤ ë²¡í„° í‘œì¤€í¸ì°¨ ë²¡í„° ì–»ê¸°
- 5.1 ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¼ê¸°íŒŒì¼ìŒì„±ìš© ì‹œí€€ìŠ¤ë²¡í„°ë¥¼ ë§Œë“ ë‹¤
- ë‘ ë²¡í„°ì˜ ì°¨ì´ê°’ì„ ëª¨ë¸ì˜ ì…ë ¥ê°’ìœ¼ë¡œ ë„£ëŠ”ë‹¤  delta ë²¡í„° = (ì¼ê¸°íŒŒì¼ ìŒì„±ìš© ë²¡í„° - ë² ì´ìŠ¤ë²¡í„° í‰ê· ) / ë² ì´ìŠ¤ë²¡í„° í‘œì¤€í¸ì°¨



### ğŸ”¥ Model card : **HyukII/audio-emotion-model**

### ğŸ”¥ Load in code:
```python
import json, torch, numpy as np
from huggingface_hub import hf_hub_download
from importlib.machinery import SourceFileLoader

repo = "HyukII/audio-emotion-model"
w = hf_hub_download(repo, "pytorch_model.pth")
m = hf_hub_download(repo, "model.py")
lab = hf_hub_download(repo, "labels.json")

labels = json.load(open(lab, encoding="utf-8"))
Model = SourceFileLoader("amodel", m).load_module().PyTorchAudioModel

model = Model(num_labels=len(labels)).eval()
state = torch.load(w, map_location="cpu")
model.load_state_dict(state)
# x: tensor (1,13,100) â†’ probs = softmax(model(x), dim=1)



